{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1BGK7v-y7TwhA3q2f2xV7EqVvnthv16Nl",
      "authorship_tag": "ABX9TyNgXmJWq9KSmHcZZV3e9G31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mku813/ActiveLearning/blob/main/AL_default_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "h_J9dU1Or_IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "from torch.utils.data import ConcatDataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models import resnet18"
      ],
      "metadata": {
        "id": "aQw0tEifVfVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "- 5000개의 train dataset으로 학습하고, 8000개의 test dataset을 10개의 class에서 100개씩 샘플링해서 총 1000개의 데이터를 Active Learining 할 수 있도록 세팅"
      ],
      "metadata": {
        "id": "gMn44tdSJXUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.STL10('/content/drive/MyDrive/CAU/2023_1/default_AL/train', split='train', download=False, transform=transforms.ToTensor())\n",
        "n_train_samples = len(train_dataset)\n",
        "train_indices = list(range(n_train_samples))\n",
        "random.shuffle(train_indices)\n",
        "n_init_samples = 1000  # 초기 학습 데이터 샘플 수\n",
        "init_indices = train_indices[:n_init_samples]\n",
        "init_subset = Subset(train_dataset, init_indices)"
      ],
      "metadata": {
        "id": "lF3sOXTqBHIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = resnet18(pretrained=True)\n",
        "num_classes = 10  # STL10 데이터셋 클래스 수\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "train_loader = DataLoader(init_subset, batch_size=32, shuffle=True)\n",
        "\n",
        "for epoch in range(10):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "XmyLxc7xNktG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = datasets.STL10('/content/drive/MyDrive/CAU/2023_1/default_AL/test', split='test', download=False, transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "acc = []\n",
        "\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100.0 * correct / total\n",
        "    return accuracy\n",
        "\n",
        "init_acc = test(model, test_loader)\n",
        "acc.append(init_acc)\n",
        "print('Initial accuracy:', init_acc)"
      ],
      "metadata": {
        "id": "vrFEThMvNkjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def uncertainty_sampling(model, loader):\n",
        "    model.eval()\n",
        "    scores = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in loader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "            softmax_outputs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            entropy = torch.sum(-softmax_outputs * torch.log2(softmax_outputs), dim=1)\n",
        "            scores.extend(entropy.cpu().numpy())\n",
        "    return scores"
      ],
      "metadata": {
        "id": "ingTdCgPNkQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iterations = 10  # 반복 횟수\n",
        "n_new_samples = 1000  # 선택할 새로운 데이터 샘플 수\n",
        "\n",
        "for iteration in range(n_iterations):\n",
        "    # 4. 새로운 데이터 선택\n",
        "    new_indices = list(set(train_indices) - set(init_indices))\n",
        "    random.shuffle(new_indices)\n",
        "    new_subset = Subset(train_dataset, new_indices[:n_new_samples])\n",
        "\n",
        "    # 5. 새로운 데이터 불확실성 계산\n",
        "    train_subset = ConcatDataset([init_subset, new_subset])\n",
        "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "    scores = uncertainty_sampling(model, train_loader)\n",
        "\n",
        "    # 6. 불확실성이 높은 새로운 데이터 선택\n",
        "    selected_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:n_new_samples]\n",
        "    selected_subset = Subset(train_dataset, [new_indices[i] for i in selected_indices])\n",
        "\n",
        "    # 7. 선택된 데이터 레이블링 (수동으로)\n",
        "    # ...\n",
        "\n",
        "    # 8. 선택된 데이터 추가\n",
        "    train_subset = ConcatDataset([train_subset, selected_subset])\n",
        "\n",
        "    # 9. 모델 재학습\n",
        "    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
        "    for epoch in range(10):\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    nth_acc = test(model, test_loader)\n",
        "    acc.append(nth_acc)\n",
        "    print('accuracy:', nth_acc)"
      ],
      "metadata": {
        "id": "HyYElj5WOt0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = [19.0625, 74.1625, 80.6125, 82.9, 84.275, 85.2, 85.875, 85.9875, 86.0, 86.1875, 86.35] \n",
        "plt.plot(acc, marker = 'o')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ehcpYwsmOtoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.mkdir('/content/drive/MyDrive/CAU/2023_1/res')\n",
        "os.mkdir('/content/drive/MyDrive/CAU/2023_1/res/models')"
      ],
      "metadata": {
        "id": "6ndm4WiFZkRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, '/content/drive/MyDrive/CAU/2023_1/res/models/00_entropy_noDiversity.model')"
      ],
      "metadata": {
        "id": "0i7Epsb8Zj9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장된 모델 불러오기\n",
        "loaded_model = torch.load('/content/drive/MyDrive/CAU/2023_1/res/models/00_entropy_noDiversity.model')"
      ],
      "metadata": {
        "id": "p5bR9nZobqyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 모델이 같은지 확인하는 코드\n",
        "import collections as co\n",
        "\n",
        "params1 = model.state_dict()\n",
        "params2 = loaded_model.state_dict()\n",
        "\n",
        "\n",
        "# OrderedDict의 모든 값을 Tensor로 변환\n",
        "tensor_dict1 = co.OrderedDict()\n",
        "for key, value in params1.items():\n",
        "    tensor_dict1[key] = torch.tensor(value)\n",
        "\n",
        "tensor_dict2 = co.OrderedDict()\n",
        "for key, value in params2.items():\n",
        "    tensor_dict2[key] = torch.tensor(value)\n",
        "\n",
        "is_equal = all(torch.equal(tensor_dict1[key], tensor_dict2[key]) for key in tensor_dict1.keys())\n",
        "\n",
        "if is_equal:\n",
        "    print(\"두 모델은 동일합니다.\")\n",
        "else:\n",
        "    print(\"두 모델은 동일하지 않습니다.\")"
      ],
      "metadata": {
        "id": "no_c8AqZcy0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "05I0eNydcyae"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}